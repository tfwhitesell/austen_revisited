{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metadata\n",
    "metadata = pd.read_csv('../data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares text into chunks (no other pre-processing) and joins to metadata\n",
    "import helper_functions as h\n",
    "\n",
    "id_list = metadata['text#']\n",
    "df_list = [] # df from each loop appended to list\n",
    "\n",
    "for id in id_list:\n",
    "    text = h.read_text(id)      # read in text\n",
    "    sent_text = sent_tokenize(text)     # tokenize text by sentence\n",
    "    token_text, token_num, token_char_count, token_sent_count, token_word_count = h.get_tokens(sent_text)    # split text into tokens with sentence structure in mind\n",
    "    df = pd.DataFrame(list(zip(token_text, token_num, token_char_count, token_sent_count, token_word_count)), \n",
    "                            columns = ['token_text', 'token_num', 'token_char_count', 'token_sent_count', 'token_word_count'])\n",
    "    df['text#'] = id\n",
    "    df_list.append(df)    # list of dataframes with token data\n",
    "\n",
    "token_df = pd.concat(df_list)\n",
    "\n",
    "token_2000 = pd.merge(metadata, token_df, on = 'text#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text#</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>words</th>\n",
       "      <th>text_length</th>\n",
       "      <th>is_Austen</th>\n",
       "      <th>is_Austen_bool</th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_num</th>\n",
       "      <th>token_char_count</th>\n",
       "      <th>token_sent_count</th>\n",
       "      <th>token_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55697983</td>\n",
       "      <td>We Have Been Trying To Reach You About Your Li...</td>\n",
       "      <td>Katri</td>\n",
       "      <td>11116</td>\n",
       "      <td>short</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. Bennet was still feeling a bit weak after ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1836</td>\n",
       "      <td>16</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55697983</td>\n",
       "      <td>We Have Been Trying To Reach You About Your Li...</td>\n",
       "      <td>Katri</td>\n",
       "      <td>11116</td>\n",
       "      <td>short</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "      <td>They may die of contagious fevers, of fevers t...</td>\n",
       "      <td>2</td>\n",
       "      <td>1962</td>\n",
       "      <td>18</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55697983</td>\n",
       "      <td>We Have Been Trying To Reach You About Your Li...</td>\n",
       "      <td>Katri</td>\n",
       "      <td>11116</td>\n",
       "      <td>short</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "      <td>And there has been a disconcerting increase in...</td>\n",
       "      <td>3</td>\n",
       "      <td>1981</td>\n",
       "      <td>21</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55697983</td>\n",
       "      <td>We Have Been Trying To Reach You About Your Li...</td>\n",
       "      <td>Katri</td>\n",
       "      <td>11116</td>\n",
       "      <td>short</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "      <td>Is there anything I can do to increase my chan...</td>\n",
       "      <td>4</td>\n",
       "      <td>1792</td>\n",
       "      <td>10</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55697983</td>\n",
       "      <td>We Have Been Trying To Reach You About Your Li...</td>\n",
       "      <td>Katri</td>\n",
       "      <td>11116</td>\n",
       "      <td>short</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "      <td>“But the frequency of the daughters travelling...</td>\n",
       "      <td>5</td>\n",
       "      <td>1960</td>\n",
       "      <td>17</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text#                                              title author  words  \\\n",
       "0  55697983  We Have Been Trying To Reach You About Your Li...  Katri  11116   \n",
       "1  55697983  We Have Been Trying To Reach You About Your Li...  Katri  11116   \n",
       "2  55697983  We Have Been Trying To Reach You About Your Li...  Katri  11116   \n",
       "3  55697983  We Have Been Trying To Reach You About Your Li...  Katri  11116   \n",
       "4  55697983  We Have Been Trying To Reach You About Your Li...  Katri  11116   \n",
       "\n",
       "  text_length   is_Austen  is_Austen_bool  \\\n",
       "0       short  Not Austen               0   \n",
       "1       short  Not Austen               0   \n",
       "2       short  Not Austen               0   \n",
       "3       short  Not Austen               0   \n",
       "4       short  Not Austen               0   \n",
       "\n",
       "                                          token_text token_num  \\\n",
       "0  Mr. Bennet was still feeling a bit weak after ...         1   \n",
       "1  They may die of contagious fevers, of fevers t...         2   \n",
       "2  And there has been a disconcerting increase in...         3   \n",
       "3  Is there anything I can do to increase my chan...         4   \n",
       "4  “But the frequency of the daughters travelling...         5   \n",
       "\n",
       "  token_char_count token_sent_count token_word_count  \n",
       "0             1836               16              312  \n",
       "1             1962               18              329  \n",
       "2             1981               21              343  \n",
       "3             1792               10              316  \n",
       "4             1960               17              357  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split id lists, use for custom train-test split\n",
    "train_ids, test_ids = h.split_by_id(metadata, 80, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB model with tfidf vectorizer to convert text data to numerical features\n",
    "\n",
    "# subset df for train and test\n",
    "train_df = token_2000[token_2000['text#'].isin(train_ids)]\n",
    "test_df = token_2000[token_2000['text#'].isin(test_ids)]\n",
    "\n",
    "X_train = train_df['token_text']\n",
    "X_test = test_df['token_text']\n",
    "y_train = train_df['is_Austen_bool']\n",
    "y_test = test_df['is_Austen_bool']\n",
    "\n",
    "# convert text to numerical features with tf-idf\n",
    "vect = TfidfVectorizer()\n",
    "X_train_vec = vect.fit_transform(X_train)\n",
    "X_test_vec = vect.transform(X_test)\n",
    "\n",
    "# train XGB model\n",
    "model = XGBClassifier(n_jobs = -1)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = model.predict(X_test_vec)\n",
    "y_pred_proba = model.predict_proba(X_test_vec)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9064665127020786"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5226,  121],\n",
       "       [ 446,  269]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not Austen       0.92      0.98      0.95      5347\n",
      "      Austen       0.69      0.38      0.49       715\n",
      "\n",
      "    accuracy                           0.91      6062\n",
      "   macro avg       0.81      0.68      0.72      6062\n",
      "weighted avg       0.89      0.91      0.89      6062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = ['not Austen', 'Austen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9113471903073548"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is still pretty good at identifying not-Austen, but very poor at identifying Austen.\n",
    "\n",
    "Change to count vectorizer, just because I'm curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB model with count vectorizer to convert text data to numerical features\n",
    "\n",
    "# subset df for train and test\n",
    "train_df = token_2000[token_2000['text#'].isin(train_ids)]\n",
    "test_df = token_2000[token_2000['text#'].isin(test_ids)]\n",
    "\n",
    "X_train = train_df['token_text']\n",
    "X_test = test_df['token_text']\n",
    "y_train = train_df['is_Austen_bool']\n",
    "y_test = test_df['is_Austen_bool']\n",
    "\n",
    "# convert text to numerical features with tf-idf\n",
    "vect = CountVectorizer()\n",
    "X_train_vec = vect.fit_transform(X_train)\n",
    "X_test_vec = vect.transform(X_test)\n",
    "\n",
    "# train XGB model\n",
    "model = XGBClassifier(n_jobs = -1)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = model.predict(X_test_vec)\n",
    "y_pred_proba = model.predict_proba(X_test_vec)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9026723853513692\n",
      "[[5237  110]\n",
      " [ 480  235]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not Austen       0.92      0.98      0.95      5347\n",
      "      Austen       0.68      0.33      0.44       715\n",
      "\n",
      "    accuracy                           0.90      6062\n",
      "   macro avg       0.80      0.65      0.70      6062\n",
      "weighted avg       0.89      0.90      0.89      6062\n",
      "\n",
      "0.9008269456371195\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names = ['not Austen', 'Austen']))\n",
    "print(roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty consistent, count performs a little worse that tf-idf as expected with no pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
