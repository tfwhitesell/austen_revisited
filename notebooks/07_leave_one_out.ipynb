{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import helper_functions as h\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts on setting up for leave one out approach:\n",
    "- Should be similar to custom split, except instead of randomly assigning all the ids to a group based on a percentage a single id will be chosen for test and the rest put into train; to iterate correctly the test id needs to be put back into train for the following runs.\n",
    "- list with unique text ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using sci-kit learn's LeaveOneGroupOut, the group would be the text id, so it should work the way I want without custom coding it?\n",
    "\n",
    "I actually think I'll understand what's going on much better if I code it. The examples using the out-of-the-box function aren't a great fit for what I want to do (and the way my data is structured) and are complicated anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('../data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text#</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>words</th>\n",
       "      <th>text_length</th>\n",
       "      <th>is_Austen</th>\n",
       "      <th>is_Austen_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55697983</td>\n",
       "      <td>We Have Been Trying To Reach You About Your Li...</td>\n",
       "      <td>Katri</td>\n",
       "      <td>11116</td>\n",
       "      <td>short</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55785940</td>\n",
       "      <td>The Younger Son</td>\n",
       "      <td>Sonetka</td>\n",
       "      <td>7164</td>\n",
       "      <td>short</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55435072</td>\n",
       "      <td>Golden</td>\n",
       "      <td>Courtney621</td>\n",
       "      <td>12066</td>\n",
       "      <td>short</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55189105</td>\n",
       "      <td>The Settlement of Lady Elliot's Piano</td>\n",
       "      <td>Gwynterys</td>\n",
       "      <td>5941</td>\n",
       "      <td>short</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53512534</td>\n",
       "      <td>A Different Bennet Family</td>\n",
       "      <td>Fanfictionfan_01_1981</td>\n",
       "      <td>34346</td>\n",
       "      <td>medium</td>\n",
       "      <td>Not Austen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text#                                              title  \\\n",
       "0  55697983  We Have Been Trying To Reach You About Your Li...   \n",
       "1  55785940                                    The Younger Son   \n",
       "2  55435072                                             Golden   \n",
       "3  55189105              The Settlement of Lady Elliot's Piano   \n",
       "4  53512534                          A Different Bennet Family   \n",
       "\n",
       "                  author  words text_length   is_Austen  is_Austen_bool  \n",
       "0                  Katri  11116       short  Not Austen               0  \n",
       "1                Sonetka   7164       short  Not Austen               0  \n",
       "2            Courtney621  12066       short  Not Austen               0  \n",
       "3              Gwynterys   5941       short  Not Austen               0  \n",
       "4  Fanfictionfan_01_1981  34346      medium  Not Austen               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to texts over 60K words\n",
    "metadata = metadata[metadata['text_length'] == 'long'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = metadata['text#']\n",
    "df_list = [] # df from each loop appended to list\n",
    "\n",
    "for id in id_list:\n",
    "    text = h.read_text(id)      # read in text\n",
    "    sent_text = sent_tokenize(text)     # tokenize text by sentence\n",
    "    token_text, token_num, token_char_count, token_sent_count, token_word_count = h.get_tokens(sent_text)    # split text into tokens with sentence structure in mind\n",
    "    df = pd.DataFrame(list(zip(token_text, token_num, token_char_count, token_sent_count, token_word_count)), \n",
    "                            columns = ['token_text', 'token_num', 'token_char_count', 'token_sent_count', 'token_word_count'])\n",
    "    df['text#'] = id\n",
    "    df_list.append(df)    # list of dataframes with token data\n",
    "\n",
    "token_df = pd.concat(df_list)\n",
    "\n",
    "token_2000 = pd.merge(metadata, token_df, on = 'text#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all but one id in train_ids, remaining in test_ids\n",
    "ids = token_2000['text#'].unique().tolist()\n",
    "\n",
    "for id in ids:\n",
    "    train_ids = list(filter(lambda x: x != id, ids))\n",
    "    test_ids = [[id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    " \n",
    "# Driver Code\n",
    "print(intersection(train_ids, test_ids))\n",
    "# confirmation it works the way I expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_2000['text#'].nunique()\n",
    "# 46 book-length texts including 6 Austen novels and 40 fanfic works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model will be built and trained inside the loop.\n",
    "\n",
    "In my mind I want to end up with validation metrics appended to the dataframe so I can see how it changes by novel and also perhaps by position in the text (since the tokens are numbered sequentially). To do that, in each iteration I have a test dataset for which predictions are being made. The prediction (proba) series can be joined to the test dataframe since they should be in the same order. Include the token_num as a column since that is a unique identifier within each text? This seems similar to how we joined the predictions onto the test data to submit for the Kaggle project.\n",
    "\n",
    "Each test dataframe should be appended to a list of dataframes and all of them concatenated together once all the iterations are done. This should result in a dataframe the same length as the original but with additional validation information.\n",
    "\n",
    "To validate: if I save the prediction as a df column, I should be able to use it to calculate other evaluation metrics I think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the previously used vectorizer model. Instead of the custom train-test split, use leave one out. For each id, generate predictions and append the predictions to the appropriate subset of the full dataframe. Add the subset dataframe to a list. At the end of the iterations concat the df into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text id: 52705351, loop: 1\n",
      "text id: 30672131, loop: 2\n",
      "text id: 40263168, loop: 3\n",
      "text id: 33964492, loop: 4\n",
      "text id: 35534191, loop: 5\n",
      "text id: 37522585, loop: 6\n",
      "text id: 34200601, loop: 7\n",
      "text id: 30497346, loop: 8\n",
      "text id: 33704434, loop: 9\n",
      "text id: 34889029, loop: 10\n",
      "text id: 27536020, loop: 11\n",
      "text id: 9401669, loop: 12\n",
      "text id: 25053859, loop: 13\n",
      "text id: 20325682, loop: 14\n",
      "text id: 16334435, loop: 15\n",
      "text id: 27446335, loop: 16\n",
      "text id: 28268808, loop: 17\n",
      "text id: 9680840, loop: 18\n",
      "text id: 24216613, loop: 19\n",
      "text id: 4102567, loop: 20\n",
      "text id: 5762899, loop: 21\n",
      "text id: 25706614, loop: 22\n",
      "text id: 24009643, loop: 23\n",
      "text id: 23907943, loop: 24\n",
      "text id: 22911136, loop: 25\n",
      "text id: 23294782, loop: 26\n",
      "text id: 21285821, loop: 27\n",
      "text id: 9832895, loop: 28\n",
      "text id: 20520542, loop: 29\n",
      "text id: 13708740, loop: 30\n",
      "text id: 11344053, loop: 31\n",
      "text id: 10896180, loop: 32\n",
      "text id: 7013194, loop: 33\n",
      "text id: 6770071, loop: 34\n",
      "text id: 1805647, loop: 35\n",
      "text id: 444476, loop: 36\n",
      "text id: 207589, loop: 37\n",
      "text id: 206617, loop: 38\n",
      "text id: 206569, loop: 39\n",
      "text id: 417, loop: 40\n",
      "text id: 105, loop: 41\n",
      "text id: 121, loop: 42\n",
      "text id: 141, loop: 43\n",
      "text id: 158, loop: 44\n",
      "text id: 21839, loop: 45\n",
      "text id: 42671, loop: 46\n"
     ]
    }
   ],
   "source": [
    "# list to iterate over\n",
    "ids = token_2000['text#'].unique().tolist()\n",
    "\n",
    "logo_list = []\n",
    "item_count = 0\n",
    "\n",
    "for id in ids:\n",
    "    train_ids = list(filter(lambda x: x != id, ids))\n",
    "    test_ids = [id]\n",
    "\n",
    "    # subset dataframe for test id\n",
    "    id_tokens = token_2000[token_2000['text#'] == id].copy()\n",
    "\n",
    "    # train-test-split\n",
    "    X_train = token_2000[token_2000['text#'].isin(train_ids)]['token_text']\n",
    "    X_test = token_2000[token_2000['text#'].isin(test_ids)]['token_text']\n",
    "    y_train = token_2000[token_2000['text#'].isin(train_ids)]['is_Austen_bool']\n",
    "    y_test = token_2000[token_2000['text#'].isin(test_ids)]['is_Austen_bool']\n",
    "\n",
    "    vect = CountVectorizer()\n",
    "\n",
    "    X_train_vec = vect.fit_transform(X_train)\n",
    "    X_test_vec = vect.transform(X_test)\n",
    "\n",
    "    nb = MultinomialNB().fit(X_train_vec, y_train)\n",
    "\n",
    "    y_pred = nb.predict(X_test_vec)\n",
    "    y_proba = nb.predict_proba(X_test_vec)\n",
    "\n",
    "    id_tokens['predictions'] = y_pred\n",
    "    id_tokens['probabilities'] = y_proba[:,1]\n",
    "\n",
    "    logo_list.append(id_tokens)\n",
    "    item_count += 1\n",
    "    print(f'text id: {id}, loop: {item_count}')\n",
    "\n",
    "logo_df = pd.concat(logo_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this on all 350 texts took 18 minutes. For the sake of getting things done, narrow the dataset to novel-length texts (already have them classified, subset for 'long').\n",
    "\n",
    "Took 1 minute to run with 46 texts, much more doable for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
